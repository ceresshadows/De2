{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd61cb78-6ef0-4f69-ab1b-a9ce4501149c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e5f2460-a942-4c13-bb7c-30b11446e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "### 处理文件路径\n",
    "root_path = 'assets/1npccutin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31016486-6060-48d5-b7a8-f04110ced3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npc_from_txt(filepath):\n",
    "    f = open(filepath, 'r')\n",
    "    fframe = f.readlines()\n",
    "    \n",
    "    #append timestamp\n",
    "    df_ts = pd.DataFrame(columns=['npc_ts'])\n",
    "    #append npc\n",
    "    df_lc = pd.DataFrame(columns=['npc_x','npc_y','theta'])\n",
    "    \n",
    "    for row in fframe:\n",
    "        if row.find('timestamp_sec') != -1:\n",
    "            ts = re.findall(\"(?<=[AZaz])?[0-9.+-]+\",row) #exclude \"timestamp_sec:\"\n",
    "            ts = float(ts[0]) #convert to data format\n",
    "            l_no = fframe.index(row)\n",
    "            \n",
    "            lc_x = re.findall(\"(?<=[AZaz])?[0-9.+-]+\",fframe[l_no + 11])\n",
    "            lc_x = float(lc_x[0])\n",
    "            lc_y = re.findall(\"(?<=[AZaz])?[0-9.+-]+\",fframe[l_no + 12])\n",
    "            lc_y = float(lc_y[0])\n",
    "            theta = re.findall(\"(?<=[AZaz])?[0-9.+-]+\",fframe[l_no +15])\n",
    "            theta = float(theta[0])\n",
    "            \n",
    "            d_0 = {'npc_ts':[ts]}\n",
    "            d_df0 = pd.DataFrame(data=d_0)\n",
    "            df_ts = pd.concat([df_ts, d_df0])\n",
    "\n",
    "            d_1 = {'npc_x':[lc_x], 'npc_y':[lc_y],'theta':[theta]}\n",
    "            d_df1 = pd.DataFrame(data=d_1)\n",
    "            df_lc = pd.concat([df_lc, d_df1])\n",
    "    df = pd.concat([df_ts, df_lc], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def npc_from_txt_wid(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    blocks = re.split(r'module_name: \"perception_obstacle\"', content)\n",
    "\n",
    "    # 创建一个空的DataFrame用于存储数据\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for block in blocks:\n",
    "        data = dict()\n",
    "        ts_match = re.search(r'timestamp_sec: ([\\d.+-]+)', block)\n",
    "        if ts_match:\n",
    "            ts = float(ts_match.group(1))   \n",
    "            data['npc_ts'] = ts\n",
    "        subblocks = re.split(r'perception_obstacle \\{', block)\n",
    "        # 计算并打印perception_obstacle的数量\n",
    "        num_perception_obstacles = len(subblocks) - 1\n",
    "        for i, subblock in enumerate(subblocks):\n",
    "            # 使用正则表达式提取所需的数据\n",
    "            id_match = re.search(r'id: (\\d+)', subblock)\n",
    "            x_match = re.search(r'position \\{\\s+x: ([\\d.+-]+)', subblock)\n",
    "            y_match = re.search(r'y: ([\\d.+-]+)', subblock)\n",
    "            theta_match = re.search(r'theta: ([\\d.+-]+)', subblock)\n",
    "            \n",
    "            # 如果在块中找到了所有所需的数据，则将其添加到DataFrame中\n",
    "            if id_match and x_match and y_match and theta_match:\n",
    "                npc_id = int(id_match.group(1))\n",
    "                x = float(x_match.group(1))\n",
    "                y = float(y_match.group(1))\n",
    "                theta = float(theta_match.group(1))\n",
    "\n",
    "                # 动态创建列名\n",
    "                data.update({\n",
    "                    f'NPC{npc_id}X': x,\n",
    "                    f'NPC{npc_id}Y': y,\n",
    "                    f'NPC{npc_id}Theta': theta,\n",
    "                })\n",
    "\n",
    "        if data:\n",
    "            # 使用pandas.concat将数据添加到DataFrame中\n",
    "            df = pd.concat([df, pd.DataFrame([data])], ignore_index=True)\n",
    "    \n",
    "    # 使用timestamp_sec列对DataFrame进行排序（如果需要的话）\n",
    "    # df = df.sort_values(by='timestamp_sec').reset_index(drop=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb2adf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ego_from_txt(filepath):\n",
    "    f = open(filepath, 'r')\n",
    "    fframe = f.readlines()\n",
    "    \n",
    "    #append timestamp\n",
    "    df_ts = pd.DataFrame(columns=['ego_ts'])\n",
    "    #append position\n",
    "    df_lc = pd.DataFrame(columns=['egoX','egoY','heading'])\n",
    "    \n",
    "    for row in fframe:\n",
    "        if row.find('timestamp_sec') != -1:\n",
    "            ts = re.findall(\"(?<=[AZaz])?[0-9.+-]+\",row) #exclude \"timestamp_sec:\"\n",
    "            ts = float(ts[0]) #convert to data format\n",
    "            l_no = fframe.index(row)\n",
    "            \n",
    "            lc_x = re.findall(\"(?<=[AZaz])?[0-9.+-]+\",fframe[l_no + 6])\n",
    "            lc_x = float(lc_x[0])\n",
    "            lc_y = re.findall(\"(?<=[AZaz])?[0-9.+-]+\",fframe[l_no + 7])\n",
    "            lc_y = float(lc_y[0])\n",
    "            heading = re.findall(\"(?<=[AZaz])?[0-9.+-]+\",fframe[l_no +31])\n",
    "            heading = float(heading[0])\n",
    "            \n",
    "            d_0 = {'ego_ts':[ts]}\n",
    "            d_df0 = pd.DataFrame(data=d_0)\n",
    "            df_ts = pd.concat([df_ts, d_df0])\n",
    "\n",
    "            d_1 = {'egoX':[lc_x], 'egoY':[lc_y],'heading':[heading]}\n",
    "            d_df1 = pd.DataFrame(data=d_1)\n",
    "            df_lc = pd.concat([df_lc, d_df1])\n",
    "    df = pd.concat([df_ts, df_lc], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "437002f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmd_from_txt(filepath):\n",
    "    f = open(filepath, 'r')\n",
    "    fframe = f.readlines()\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame(columns=['cmd_ts', 'throttle', 'brake', 'steering'])\n",
    "    \n",
    "    for l_no, row in enumerate(fframe):\n",
    "        if re.match(r'^header {', row):\n",
    "            ts = re.findall(\"(?<=[AZaz])?[0-9.+-]+\", fframe[l_no + 1])\n",
    "            ts = float(ts[0])\n",
    "\n",
    "            throttle = re.findall(\"(?<=[AZaz])?[0-9.+-]+\", fframe[l_no + 11])\n",
    "            if throttle:\n",
    "                throttle = float(throttle[0])\n",
    "            else:\n",
    "                throttle = re.findall(\"(?<=[AZaz])?[0-9.+-]+\", fframe[l_no + 12])\n",
    "                if throttle:\n",
    "                    throttle = float(throttle[0])\n",
    "                else:\n",
    "                    print(f\"Error in {filepath}, line {l_no}: Throttle value not found\")\n",
    "                    print(f\"timestamp = {ts}\")\n",
    "\n",
    "            # throttle = float(throttle[0])\n",
    "\n",
    "            brake = re.findall(\"(?<=[AZaz])?[0-9.+-]+\", fframe[l_no + 12])\n",
    "            brake = float(brake[0])\n",
    "\n",
    "            steering = re.findall(\"(?<=[AZaz])?[0-9.+-]+\", fframe[l_no + 14])\n",
    "            steering = float(steering[0])\n",
    "\n",
    "            col_df = pd.concat([pd.DataFrame({'cmd_ts': [ts]}), \n",
    "                                 pd.DataFrame({'throttle': [throttle]}), \n",
    "                                 pd.DataFrame({'brake': [brake]}), \n",
    "                                 pd.DataFrame({'steering': [steering]})], axis=1)\n",
    "            \n",
    "            df = pd.concat([df, col_df], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5abb13d4-7044-416d-bc99-40cbe1866180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_average(serial):\n",
    "    data_path = root_path\n",
    "    df_npc = npc_from_txt_wid(data_path+f'/txt_record/{serial}_obstacle.txt')\n",
    "    df_ego = ego_from_txt(data_path+f'/txt_record/{serial}_pose.txt')\n",
    "    df_cmd = cmd_from_txt(data_path+f'/txt_record/{serial}_control.txt')\n",
    "## 根据是否是obstacle来的选择要不要手动对齐数据\n",
    "    delta_ts = df_ego['ego_ts'].iloc[0] - df_npc['npc_ts'].iloc[0]\n",
    "    df_npc['npc_ts'] += delta_ts\n",
    "\n",
    "    # 使用列表来收集有效的数据\n",
    "    aligned_ego_list = []\n",
    "    avg_cmd_list = []\n",
    "    aligned_npc_list = []\n",
    "    \n",
    "    # 上一个npc时间戳的索引\n",
    "    last_npc_idx = 0\n",
    "    \n",
    "    for i in range(df_npc.shape[0]):\n",
    "        # 对齐ego的时间戳\n",
    "        ego_indices = np.where(df_ego['ego_ts'] < df_npc['npc_ts'].iloc[i])[0]\n",
    "        if ego_indices.size == 0:\n",
    "            continue\n",
    "        ego_idx = ego_indices[-1]\n",
    "        \n",
    "        # 平均cmd的数据\n",
    "        cmd_indices = np.where((df_cmd['cmd_ts'] >= df_npc['npc_ts'].iloc[last_npc_idx]) & \n",
    "                               (df_cmd['cmd_ts'] < df_npc['npc_ts'].iloc[i]))[0]\n",
    "        \n",
    "        # 检查cmd_indices是否为空\n",
    "        if cmd_indices.size > 0:\n",
    "            aligned_ego_list.append(df_ego.iloc[ego_idx])\n",
    "            avg_cmd_list.append(df_cmd.iloc[cmd_indices].mean())\n",
    "            aligned_npc_list.append(df_npc.iloc[i])\n",
    "        \n",
    "        last_npc_idx = i\n",
    "\n",
    "    # 将列表转换为DataFrame\n",
    "    df_aligned_ego = pd.DataFrame(aligned_ego_list).reset_index(drop=True)\n",
    "    df_avg_cmd = pd.DataFrame(avg_cmd_list).reset_index(drop=True)\n",
    "    df_aligned_npc = pd.DataFrame(aligned_npc_list).reset_index(drop=True)\n",
    "\n",
    "    return df_aligned_ego, df_aligned_npc, df_avg_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "616cbfdd-7aba-482e-9cbc-6dd3f3d2b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试用的，单独提取一个run的record\n",
    "# aligned_ego, aligned_npc, avg_cmd = align_and_average(46)\n",
    "\n",
    "# # print (aligned_ego)\n",
    "# # print (avg_cmd)\n",
    "# # 将numpy数组转换为pandas DataFrame\n",
    "# df_ego = pd.DataFrame(aligned_ego)  # 请根据你的数据调整列名\n",
    "# df_cmd = pd.DataFrame(avg_cmd)     # 请根据你的数据调整列名\n",
    "# df_npc = pd.DataFrame(aligned_npc)         # 请根据你的数据调整列名\n",
    "\n",
    "# # 保存为CSV文件\n",
    "# df_ego.to_csv('assets/data200/aligned_ego.csv', index=False)\n",
    "# df_cmd.to_csv('assets/data200/avg_cmd.csv', index=False)\n",
    "# df_npc.to_csv('assets/data200/aligned_npc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "674bc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单独处理的46\n",
    "def process_serials(serials):\n",
    "    merge_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(serials):\n",
    "        # if i == 46:\n",
    "        #     continue\n",
    "        print(f\"Processing serial {i}\")\n",
    "        aligned_ego, aligned_npc, avg_cmd = align_and_average(i)\n",
    "        # 创建一个临时的DataFrame来存储这一轮的数据\n",
    "        tmp = aligned_ego['ego_ts'].to_frame(name='time')\n",
    "        tmp['id'] = i + 1  # 添加新列\n",
    "        tmp = pd.concat([tmp, aligned_ego, aligned_npc, avg_cmd], axis=1)\n",
    "        \n",
    "        # 在尝试删除列之前，检查它们是否存在\n",
    "        columns_to_drop = ['ego_ts', 'cmd_ts', 'npc_ts']\n",
    "        for col in columns_to_drop:\n",
    "            if col not in tmp.columns:\n",
    "                print(f\"Warning: Column '{col}' not found in DataFrame at iteration {i}. Available columns: {tmp.columns}\")\n",
    "            else:\n",
    "                tmp = tmp.drop(columns=col)\n",
    "        # 将这一轮的数据添加到merge_df中\n",
    "        merge_df = pd.concat([merge_df, tmp], ignore_index=True)\n",
    "    return merge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f1da9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里是提取静态feature的部分\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 定义一个函数来解析文件并返回一个DataFrame\n",
    "def parse_simulation_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # 初始化空的列表来存储数据\n",
    "    data = []\n",
    "    \n",
    "    # 初始化一个空的字典来临时存储每次迭代的数据\n",
    "    iteration_data = {}\n",
    "    \n",
    "    # 遍历文件的每一行\n",
    "    for line in lines:\n",
    "        # 使用正则表达式来解析数据\n",
    "        if \"Iteration\" in line:\n",
    "            # 如果字典不为空，则将其添加到数据列表中，并清空字典\n",
    "            if iteration_data:\n",
    "                data.append(iteration_data)\n",
    "                iteration_data = {}\n",
    "            iteration_data['id'] = int(re.search(r'\\d+', line).group())\n",
    "        elif \"Weather\" in line:\n",
    "            weather_data = re.findall(r'(\\w+)=([\\d.]+)', line)\n",
    "            for item in weather_data:\n",
    "                iteration_data[item[0]] = float(item[1])\n",
    "        elif \"Time of day\" in line:\n",
    "            iteration_data['daytime'] = float(re.search(r'[\\d.]+', line).group())\n",
    "        elif \"Collision\" in line:\n",
    "            match = re.search(r\"Collision: (True|False)\", line)\n",
    "            if match:\n",
    "                # 如果找到了匹配的字符串，获取True或False的值\n",
    "                value = match.group(1)\n",
    "                # 根据True或False的值进行转换，并保存在字典中\n",
    "                iteration_data['result'] = 1 if value == \"True\" else 0\n",
    "    \n",
    "    # 确保最后一次迭代的数据也被添加到数据列表中\n",
    "    if iteration_data:\n",
    "        data.append(iteration_data)\n",
    "    \n",
    "    # 将数据列表转换为DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 提取结果等静态部分 使用函数解析文件并获取DataFrame\n",
    "file_path = root_path+'simulation_data.txt'\n",
    "df = parse_simulation_data(file_path)\n",
    "\n",
    "# 显示DataFrame的前几行\n",
    "# print(df)\n",
    "df.to_csv(root_path+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b618be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing serial 0\n",
      "Processing serial 1\n",
      "Processing serial 2\n",
      "Processing serial 3\n",
      "Processing serial 4\n",
      "Processing serial 5\n",
      "Processing serial 6\n",
      "Processing serial 7\n",
      "Processing serial 8\n",
      "Processing serial 9\n",
      "Processing serial 10\n",
      "Processing serial 11\n",
      "Processing serial 12\n",
      "Processing serial 13\n",
      "Processing serial 14\n",
      "Processing serial 15\n",
      "Processing serial 16\n",
      "Processing serial 17\n",
      "Processing serial 18\n",
      "Processing serial 19\n",
      "Processing serial 20\n",
      "Processing serial 21\n",
      "Processing serial 22\n",
      "Processing serial 23\n",
      "Processing serial 24\n",
      "Processing serial 25\n",
      "Processing serial 26\n",
      "Processing serial 27\n",
      "Processing serial 28\n",
      "Processing serial 29\n",
      "Processing serial 30\n",
      "Processing serial 31\n",
      "Processing serial 32\n",
      "Processing serial 33\n",
      "Processing serial 34\n",
      "Processing serial 35\n",
      "Processing serial 36\n",
      "Processing serial 37\n",
      "Processing serial 38\n",
      "Processing serial 39\n",
      "Processing serial 40\n",
      "Processing serial 41\n",
      "Processing serial 42\n",
      "Processing serial 43\n",
      "Processing serial 44\n",
      "Processing serial 45\n",
      "Processing serial 46\n",
      "Processing serial 47\n",
      "Processing serial 48\n",
      "Processing serial 49\n",
      "Processing serial 50\n",
      "Processing serial 51\n",
      "Processing serial 52\n",
      "Processing serial 53\n",
      "Processing serial 54\n",
      "Processing serial 55\n",
      "Processing serial 56\n",
      "Processing serial 57\n",
      "Processing serial 58\n",
      "Processing serial 59\n",
      "Processing serial 60\n",
      "Processing serial 61\n",
      "Processing serial 62\n",
      "Processing serial 63\n",
      "Processing serial 64\n",
      "Processing serial 65\n",
      "Processing serial 66\n",
      "Processing serial 67\n",
      "Processing serial 68\n",
      "Processing serial 69\n",
      "Processing serial 70\n"
     ]
    }
   ],
   "source": [
    "### 此处点击运行以上所有代码即可！！！修改总共的序列个数！！\n",
    "# print(process_serials(6))\n",
    "df_merge = pd.DataFrame()\n",
    "df_merge = process_serials(71)\n",
    "df_merge = df_merge.reindex(columns=['id', 'time'] + [col for col in df_merge.columns if col not in ['id', 'time']])\n",
    "df_merge.insert(0, \"\", range(1, len(df_merge) + 1))\n",
    "df_merge.to_csv(root_path+'data_processed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da28629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point (592418.5617675781, 4134988.5171813965) is on lane 260 with a distance of 0.0 to the central curve.\n"
     ]
    }
   ],
   "source": [
    "### 如果是直道场景，\n",
    "\n",
    "    \n",
    "def compute_perpendicular_distance(x, y, line):\n",
    "    x, y = float(x), float(y)  # Convert x and y to float\n",
    "    min_distance = float('inf')\n",
    "    sign = 1  # Positive for left, negative for right\n",
    "    \n",
    "    for i in range(len(line) - 1):\n",
    "        x1, y1 = float(line[i]['x']), float(line[i]['y'])  # Convert string to float\n",
    "        x2, y2 = float(line[i+1]['x']), float(line[i+1]['y'])  # Convert string to float\n",
    "        \n",
    "        v_length = math.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "        dot_product = ((x-x1)*(x2-x1) + (y-y1)*(y2-y1)) / v_length\n",
    "        \n",
    "        if dot_product < 0:\n",
    "            distance = math.sqrt((x-x1)**2 + (y-y1)**2)\n",
    "        elif dot_product > v_length:\n",
    "            distance = math.sqrt((x-x2)**2 + (y-y2)**2)\n",
    "        else:\n",
    "            # Compute the cross product to determine the side\n",
    "            cross_product = (x - x1) * (y2 - y1) - (y - y1) * (x2 - x1)\n",
    "            if cross_product < 0:\n",
    "                sign = -1\n",
    "            distance = abs((y2-y1)*x - (x2-x1)*y + x2*y1 - y2*x1) / v_length\n",
    "        \n",
    "        # Update the minimum distance with the correct sign\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance * sign\n",
    "            \n",
    "    return min_distance\n",
    "\n",
    "\n",
    "def find_lane_and_distance(x, y, lanes):\n",
    "    if x and y:\n",
    "        x, y = float(x), float(y)  # Convert x and y to float\n",
    "    else:\n",
    "        return None, None\n",
    "    min_distance = float('inf')\n",
    "    closest_lane_id = None\n",
    "    \n",
    "    for lane_id, lane_data in lanes.items():\n",
    "        central_curve = lane_data['central_curve']\n",
    "        distance = compute_perpendicular_distance(x, y, central_curve)\n",
    "        \n",
    "        # 更新最近的车道和距离\n",
    "        if abs(distance) < abs(min_distance):\n",
    "            min_distance = distance\n",
    "            closest_lane_id = lane_id\n",
    "          \n",
    "    # 如果最近的车道距离小于2m，则返回该车道和距离，否则返回None\n",
    "    if abs(min_distance) < 2:\n",
    "        lane_number = re.findall(r'\\d+', closest_lane_id)\n",
    "        return lane_number[0] if lane_number else closest_lane_id, min_distance\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "# # test\n",
    "# with open('assets/lane_info.json', 'r') as f:\n",
    "#      lanes = json.load(f)\n",
    "# x, y = 592418.56176757812, 4134988.5171813965  # Example point\n",
    "# lane_id, distance = find_lane_and_distance(x, y, lanes)\n",
    "\n",
    "# if lane_id:\n",
    "#     print(f\"The point ({x}, {y}) is on lane {lane_id} with a distance of {distance} to the central curve.\")\n",
    "# else:\n",
    "#     print(lane_id, distance)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(root_path+'data_processed.csv')\n",
    "# Load lanes data from JSON\n",
    "with open('assets/lane_info.json', 'r') as f:\n",
    "    lanes = json.load(f)\n",
    "# 遍历dataframe的每一行\n",
    "for index, row in df.iterrows():\n",
    "    # 对ego进行计算\n",
    "    lane_id, distance = find_lane_and_distance(row['egoX'], row['egoY'],lanes)\n",
    "    df.at[index, 'egoLane'] = lane_id\n",
    "    df.at[index, 'egoLaneOffset'] = distance\n",
    "    \n",
    "    # 对NPC1进行计算\n",
    "    lane_id, distance = find_lane_and_distance(row['NPC1X'], row['NPC1Y'],lanes)\n",
    "    df.at[index, 'NPC1Lane'] = lane_id\n",
    "    df.at[index, 'NPC1LaneOffset'] = distance\n",
    "    \n",
    "    # 对NPC2进行计算\n",
    "    lane_id, distance = find_lane_and_distance(row['NPC2X'], row['NPC2Y'],lanes)\n",
    "    df.at[index, 'NPC2Lane'] = lane_id\n",
    "    df.at[index, 'NPC2LaneOffset'] = distance\n",
    "\n",
    "# 将更新后的dataframe保存回CSV文件\n",
    "df.to_csv(root_path+'data_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fbb3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 如果录制的时候cut不及时,强行处理一下截断后面的：\n",
    "#读一下df.to_csv(root_path+'features.csv')如果对应result是0的，就把(root_path+'data_processed.csv',index=False)的time50之后的都不要了\n",
    "import pandas as pd\n",
    "root_path = 'assets/1failtoyield/'\n",
    "# 读取 features.csv 文件\n",
    "features_df = pd.read_csv(root_path + 'features.csv')\n",
    "\n",
    "# 找到 result 列为 0 的所有行的 id\n",
    "ids_to_remove = features_df[features_df['result'] == 0]['id']\n",
    "\n",
    "# 读取 data_processed.csv 文件\n",
    "data_processed_df = pd.read_csv(root_path + 'data_processed.csv')\n",
    "\n",
    "# 创建一个空 DataFrame 用于存储修改后的数据\n",
    "data_processed_modified_df = pd.DataFrame()\n",
    "\n",
    "for id in data_processed_df['id'].unique():\n",
    "    if id in ids_to_remove.values:\n",
    "        # 如果 id 的 result 为 0，只保留前 50 行\n",
    "        data_to_add = data_processed_df[data_processed_df['id'] == id].head(50)\n",
    "    else:\n",
    "        # 如果 id 的 result 为 1，保留所有行\n",
    "        data_to_add = data_processed_df[data_processed_df['id'] == id]\n",
    "    \n",
    "    data_processed_modified_df = pd.concat([data_processed_modified_df, data_to_add])\n",
    "\n",
    "# 删除 \"Unnamed: 0\" 列\n",
    "if 'Unnamed: 0' in data_processed_modified_df.columns:\n",
    "    data_processed_modified_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# 重置索引，使其从 1 开始\n",
    "data_processed_modified_df.index = range(1, len(data_processed_modified_df) + 1)\n",
    "\n",
    "# 保存 DataFrame，索引列没有列名\n",
    "data_processed_modified_df.to_csv(root_path + 'data_processed_modified.csv', index=True, index_label='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c67daac-c247-47ab-928e-f8614c6e0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"以下discarded: numpy overview\"\"\"\n",
    "# def process_serial(serial):\n",
    "#     aligned_ego, aligned_npc, avg_cmd = align_and_average(serial)\n",
    "\n",
    "#     # 生成具体numpy元素\n",
    "#     data_dict = {\n",
    "#         'timestamp': aligned_ego[:, 0],\n",
    "#         'ego_x': aligned_ego[:, 1],\n",
    "#         'ego_y': aligned_ego[:, 2],\n",
    "#         'ego_theta': aligned_ego[:, 3],\n",
    "#         'npc_x': aligned_npc[:, 1],\n",
    "#         'npc_y': aligned_npc[:, 2],\n",
    "#         'npc_theta': aligned_npc[:, 3],\n",
    "#         'throttle': avg_cmd[:, 1],\n",
    "#         'brake': avg_cmd[:, 2],\n",
    "#         'steering': avg_cmd[:, 3]\n",
    "#     }\n",
    "#     ## 这里是生成overview用的numpy长度\n",
    "#     # data_dict = {\n",
    "#     #     'timestamp': len(aligned_ego[:, 0]),\n",
    "#     #     'ego_x': len(aligned_ego[:, 1]),\n",
    "#     #     'ego_y': len(aligned_ego[:, 2]),\n",
    "#     #     'ego_theta': len(aligned_ego[:, 3]),\n",
    "#     #     'npc_x': len(aligned_npc[:, 1]),\n",
    "#     #     'npc_y': len(aligned_npc[:, 2]),\n",
    "#     #     'npc_theta': len(aligned_npc[:, 3]),\n",
    "#     #     'throttle': len(avg_cmd[:, 1]),\n",
    "#     #     'brake': len(avg_cmd[:, 2]),\n",
    "#     #     'steering': len(avg_cmd[:, 3])\n",
    "#     # }\n",
    "#     return data_dict\n",
    "\n",
    "# # 确保目录存在\n",
    "# output_dir = 'assets/numpy'\n",
    "\n",
    "# df_big = pd.DataFrame()  # 假设这是你的大dataframe\n",
    "\n",
    "# for serial in range(1, 22):\n",
    "#     serial_data = process_serial(serial)\n",
    "    \n",
    "#     # 为每个numpy数组保存一个文件，并在serial_data中保存文件名作为占位符\n",
    "#     for key, array in serial_data.items():\n",
    "#         filename = os.path.join(output_dir, f'output_data_{serial}_{key}.txt').replace('\\\\', '/')\n",
    "#         np.savetxt(filename, array)\n",
    "#         serial_data[key] = filename  # 保存文件名作为占位符\n",
    "        \n",
    "#     # 将serial_data添加到大dataframe中\n",
    "#     df_big = pd.concat([df_big, pd.DataFrame([serial_data])], ignore_index=True)\n",
    "\n",
    "# df_big.to_csv('assets/output_overview_np.csv', index=False)\n",
    "\n",
    "\n",
    "# # 下面的方法是把numpy数组转换成str存储在csv的！！效率比较低！\n",
    "# # for serial in range(1, 22):\n",
    "# #     serial_data = process_serial(serial)\n",
    "# #     # 将serial_data添加到大dataframe中\n",
    "# #     df_big = pd.concat([df_big, pd.DataFrame([serial_data])], ignore_index=True)\n",
    "\n",
    "# # df_big.to_csv('output_overview.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f84427",
   "metadata": {},
   "source": [
    "以上是record的预处理，结果在assets\\output_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b083275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils.txt_np import placeholder_to_np, np_to_placeholder\n",
    "\n",
    "def compute_perpendicular_distance(x, y, line):\n",
    "    x, y = float(x), float(y)  # Convert x and y to float\n",
    "    min_distance = float('inf')\n",
    "    sign = 1  # Positive for left, negative for right\n",
    "    \n",
    "    for i in range(len(line) - 1):\n",
    "        x1, y1 = float(line[i]['x']), float(line[i]['y'])  # Convert string to float\n",
    "        x2, y2 = float(line[i+1]['x']), float(line[i+1]['y'])  # Convert string to float\n",
    "        \n",
    "        v_length = math.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "        dot_product = ((x-x1)*(x2-x1) + (y-y1)*(y2-y1)) / v_length\n",
    "        \n",
    "        if dot_product < 0:\n",
    "            distance = math.sqrt((x-x1)**2 + (y-y1)**2)\n",
    "        elif dot_product > v_length:\n",
    "            distance = math.sqrt((x-x2)**2 + (y-y2)**2)\n",
    "        else:\n",
    "            # Compute the cross product to determine the side\n",
    "            cross_product = (x - x1) * (y2 - y1) - (y - y1) * (x2 - x1)\n",
    "            if cross_product < 0:\n",
    "                sign = -1\n",
    "            distance = abs((y2-y1)*x - (x2-x1)*y + x2*y1 - y2*x1) / v_length\n",
    "        \n",
    "        # Update the minimum distance with the correct sign\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance * sign\n",
    "            \n",
    "    return min_distance\n",
    "\n",
    "\n",
    "def find_lane_and_distance(x, y, lanes):\n",
    "    x, y = float(x), float(y)  # Convert x and y to float\n",
    "    min_distance = float('inf')\n",
    "    closest_lane_id = None\n",
    "    \n",
    "    for lane_id, lane_data in lanes.items():\n",
    "        central_curve = lane_data['central_curve']\n",
    "        distance = compute_perpendicular_distance(x, y, central_curve)\n",
    "        \n",
    "        # 更新最近的车道和距离\n",
    "        if abs(distance) < abs(min_distance):\n",
    "            min_distance = distance\n",
    "            closest_lane_id = lane_id\n",
    "          \n",
    "    # 如果最近的车道距离小于2m，则返回该车道和距离，否则返回None\n",
    "    if abs(min_distance) < 2:\n",
    "        return closest_lane_id, min_distance\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# test\n",
    "# x, y = 592960.0, 4135065.0  # Example point\n",
    "# lane_id, distance = find_lane_and_distance(x, y, lanes)\n",
    "\n",
    "# if lane_id:\n",
    "#     print(f\"The point ({x}, {y}) is on lane {lane_id} with a distance of {distance} to the central curve.\")\n",
    "# else:\n",
    "#     print(f\"The point ({x}, {y}) is not on any lane.\")\n",
    "\n",
    "# 接收一行的数据，返回当前一行的结果\n",
    "def process_data(lanes, ego_x_array, ego_y_array, npc_x_array, npc_y_array):\n",
    "    ego_lane_list, ego_offset_list = [], []\n",
    "    npc_lane_list, npc_offset_list = [], []\n",
    "        \n",
    "    # 迭代每个numpy数组中的每个值\n",
    "    for ego_x, ego_y, npc_x, npc_y in zip(ego_x_array, ego_y_array, npc_x_array, npc_y_array):\n",
    "        ego_lane, ego_offset = find_lane_and_distance(ego_x, ego_y, lanes)\n",
    "        npc_lane, npc_offset = find_lane_and_distance(npc_x, npc_y, lanes)\n",
    "        \n",
    "        ego_lane_list.append(ego_lane)\n",
    "        ego_offset_list.append(ego_offset)\n",
    "        npc_lane_list.append(npc_lane)\n",
    "        npc_offset_list.append(npc_offset)\n",
    "    \n",
    "    return ego_lane_list, ego_offset_list, npc_lane_list, npc_offset_list\n",
    "\n",
    "\n",
    "# Load lanes data from JSON\n",
    "with open('assets/lane_info.json', 'r') as f:\n",
    "    lanes = json.load(f)\n",
    "\n",
    "# 从CSV中读取数据\n",
    "data = pd.read_csv('assets/output_overview_np.csv')\n",
    "\n",
    "# Convert placeholders to numpy arrays\n",
    "keys = [\"ego_x\", \"ego_y\", \"npc_x\", \"npc_y\"]\n",
    "data = placeholder_to_np(data, keys)\n",
    "# print(f\"data1 = {data}\")\n",
    "\n",
    "# 确保列存在\n",
    "for col in ['ego_lane', 'ego_offset', 'npc_lane', 'npc_offset']:\n",
    "    if col not in data.columns:\n",
    "        data[col] = np.nan\n",
    "\n",
    "output_dir = 'assets/numpy'\n",
    "# 在循环中处理每个序列\n",
    "for index, row in data.iterrows():\n",
    "    ego_lane_series, ego_offset_series, npc_lane_series, npc_offset_series = process_data(\n",
    "        lanes, \n",
    "        np.array(row['ego_x']),  # Ensure it's a numpy array\n",
    "        np.array(row['ego_y']),  # Ensure it's a numpy array\n",
    "        np.array(row['npc_x']),  # Ensure it's a numpy array\n",
    "        np.array(row['npc_y']))  # Ensure it's a numpy array\n",
    "\n",
    "    # 为每个序列创建一个文件并保存\n",
    "    filenames = {}\n",
    "    for key, series in zip(['ego_lane', 'ego_offset', 'npc_lane', 'npc_offset'], \n",
    "                        [ego_lane_series, ego_offset_series, npc_lane_series, npc_offset_series]):\n",
    "        filename = os.path.join(output_dir, f'{key}_{index+1}.txt').replace('\\\\', '/')\n",
    "        if isinstance(series[0], (int, float, np.number)):  # Check if the first element is a number\n",
    "            np.savetxt(filename, series, fmt='%s')  # Save as numbers\n",
    "        else:\n",
    "            with open(filename, 'w') as f:\n",
    "                for item in series:\n",
    "                    f.write(\"%s\\n\" % item)  # Save as strings\n",
    "        filenames[key] = filename\n",
    "\n",
    "    # 在DataFrame中存储文件的名称\n",
    "    for key, filename in filenames.items():\n",
    "        data.at[index, key] = filename\n",
    "\n",
    "\n",
    "# 保存更新后的dataframe\n",
    "# data.to_csv('assets/output_lane_interaction.csv', index=False)\n",
    "\n",
    "# 1. 读取assets\\output_overview_np.csv到一个新的DataFrame\n",
    "overview_df = pd.read_csv('assets/output_overview_np.csv')\n",
    "\n",
    "# 2. 使用pd.concat将新列并入这个DataFrame\n",
    "# 确保两个DataFrame的索引是对齐的\n",
    "combined_df = pd.concat([overview_df, data[['ego_lane', 'ego_offset', 'npc_lane', 'npc_offset']]], axis=1)\n",
    "\n",
    "# 3. 将结果保存回assets\\output_overview_np.csv\n",
    "combined_df.to_csv('assets/output_overview_np.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14545370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse line: None in file: assets/numpy/ego_lane_1.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_1.txt\n",
      "Cannot parse line: None in file: assets/numpy/npc_lane_1.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_2.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_3.txt\n",
      "Cannot parse line: None in file: assets/numpy/npc_lane_3.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_4.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_5.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_6.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_6.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_7.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_7.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_9.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_11.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_14.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_15.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_15.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_16.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_16.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_17.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_17.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_18.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_18.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_19.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_20.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_20.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_21.txt\n",
      "Cannot parse line: None in file: assets/numpy/ego_lane_21.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('assets/output_overview_np.csv')\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    row_df = pd.DataFrame()\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            with open(row[column], 'r') as file:\n",
    "                data_lines = file.readlines()\n",
    "                # print(data_lines)\n",
    "                elem = [float(line.strip()) for line in data_lines if re.match(r'^-?\\d+(\\.\\d+)?(e[+-]?\\d+)?$', line.strip())]              \n",
    "                # print(elem)\n",
    "                # 如果elem列表为空，引发一个异常\n",
    "                if not elem:\n",
    "                    raise ValueError(f\"Cannot convert content of file {row[column]} to float\")\n",
    "                col_df = pd.DataFrame({column: elem})\n",
    "            # print(col_df)\n",
    "        except:\n",
    "            # print(\"!!\")\n",
    "            with open(row[column], 'r') as file:\n",
    "                data_lines = file.readlines()\n",
    "                elem = [] \n",
    "                for line in data_lines:\n",
    "                    match = re.search(r'lane_(\\d+)', line)\n",
    "                    # print(match)\n",
    "                    # break\n",
    "                    if match:\n",
    "                        elem.append(int(match.group(1)))\n",
    "                    else:\n",
    "                        try:\n",
    "                            # 尝试将其转换为浮点数\n",
    "                            elem.append(float(line.strip()))\n",
    "                        except ValueError:\n",
    "                            elem.append('nan')\n",
    "                            print(f\"Cannot parse line: {line.strip()} in file: {row[column]}\")\n",
    "        col_df = pd.DataFrame({column: elem})\n",
    "        # print(col_df)\n",
    "        # break\n",
    "        # 将col_df加到row_df\n",
    "        if 'id' in row_df:\n",
    "            row_df = pd.concat([row_df, col_df], axis=1)\n",
    "        else:\n",
    "            row_df = col_df\n",
    "            row_df['id'] = [index+1] * len(elem)  # 初始化id列\n",
    "    # 将row_df加到new_df\n",
    "    new_df = pd.concat([new_df, row_df], axis=0)\n",
    "    # print(new_df)\n",
    "\n",
    "new_df.insert(0, 'sequence', range(1, 1 + len(new_df)))\n",
    "# 保存新的数据集\n",
    "new_df.to_csv('transformed_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838cc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
